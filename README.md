# ExploreExploit

## Team Members:
    - Logan Liddiard
    - Joel Pierson

## Usage

`python main.py`

## Requirements:

    numpy
    matplotlib

These can be installed by running:
```
pip install numpy matplotlib
```

## Part 1

The best-performing epsilon value in our tests was .01.

We believe this is because once it has found the best bandit, it is much less likely to explore and go somewhere less optimal.
On the flip side of this coin, the value that seems to perfom the worst is an epsilon value of .4. This is simply because it is much more likley to explore even after it is found what it beleives is the best reward. This Leads to more often choosing poor decisions, even though it has found the best solution.

We can see that these algorithms all converge to their average reward fairly quickly. The greedy algorithm is dedicated to exploiting what it knows rather than exploring. This alogirthm doesn't explore all that much before sticking to what it thinks is the best.

Here is the output of one of our tests to prove this:

<p align="center">
  <img src="Epsilon_Test.png" alt="Epsilon Graph">
</p>

An alternative algorithm to this, is the Thompson algorithm. However, it still doesn't perform as well compared to our best performing Epsilon algorithm. This is because 10,000 steps is much shorter comparativley than running this algorithm with a larger number of steps. The Thompson Sampling algorithm is made for long term decisions. It finds which probability has the most successes over failures. This algorithm tends to explore a bit more and focus more on what has given it the most successes. Finding these successors however, takes time. This can be seen by the flatter convergance curve on the Thompson Graph.

Here is a graph comparing our Epsilon Greedy algorithm of 0.01 vs. our Thompson Sampling algorithm:

<p align="center">
  <img src="Thompson_Test.png" alt="Thomspon+Epsilon Graph">
</p>

We decided to experiment with this algorithm with both 50,000 steps and 100,000 steps and over these longer periods of time, the Thompson sampling algorithm exhibits greater performance over the best Epsilon Greedy algorithms.

## Part 2

We implemented different quenching strategies for the Epsilon Greedy Algorithms. This was the result:

<p align="center">
  <img src="Quench_Test.png" alt="Quench Graph">
</p>

We can see that linear Quenching explores too frequently. By constanty switching what reward system to use, it averages out pretty poorly for its average rewards it gets.

Asymptotic Quenching shows better performance. We see that the rewards are fairly high but the convergence to this average reward is flatter than heavy asymptotic quenching. 

Heavy Asymptotic Quenching seems to perform the best. It's convergence rate is much steeper and it performs better overall. Showing us it has a good balance of exploring and exploiting. It also shows us that it's much quicker than the other algorithms at finding this since it converges in less steps than the others.

## Part 3

In this problem, we will simulate market-like conditions by having our algorithms undergo sudden shifts. First, our mean decreasing by -0.001 at each step and at step 3,000 the following will occur:
* Bandit 0: Mean shifts by + 7
* Bandit 2: Mean shifts by + 3
* Bandit 7: Mean shifts by + 1, and if more than 3 standard deviations away, return a reward of 50.
* Bandit 18: Mean shifts by + 2.

The way we interpreted this is that suddenly at step 3,000 the bandit's mean will shift by their respective amounts, but for bandit 7, **any time after step 3000**, if more than 3 standard deviations way, return 50. This leads to very interesting results.

<p align="center">
  <img src="Epsilon_Moving_Test.png" alt="Moving Bandits Epsilon Test">  
</p>

Above we can see how the Epsilon Greedy algorithm performs with our metrics. Unlike part 1, the epsilon value of 0.01 performed much more poorly than the other values besides 0.4. This shows the potential drawback to such a low exploration rate, as exploring 1% of the time was unsucessful in finding the ideal bandit after the sudden shifts at step 3000. If we ran the test for longer, it would eventually perform the best, but this illustrates the problem with very low exploring rates. We can also see the detrimental effects of exploring too much with value 0.4. Exploring 40% of the time lead to a far less positive trend when finding the best solution, and with every mean decreasing over time, 40% of the time choosing the wrong solution can lead to significant drops in cumulative average reward. In this simulation, 0.05 and 0.1 were the perfect balance over 10,000 steps boasting a -1.363 and -1.599 average reward respectively. Considering "the house always wins" for this test (due to the decreasing means), this was rather impressive as they were able to mitigate the decreasing odds far better than 0.01 and 0.4 (which had scores of -3.049 and -3.251 respectively). Essentially, 0.4 can not reap the benefits of finding the best solution because it explores 40% of the time even after finding the best solution, which diminishes the benefits of finding the best solution in the first place. For 0.01, it struggles at finding the best solution in the first place, but once it does it performs the best. Meanwhile, 0.05 and 0.1 benefit from having moderate odds of finding the best solution and reaping the benefits by not exploring as often once it is found.

<p align="center">
  <img src="Thompson_Moving_Test.png" alt="Moving bandits Thompson">
</p>

Above we can see how the Thompson Sampling algorithm performs with our metrics. We measured the differences between reseting the algorithm and not. It appears that reseting helped the model perform much better after 3000. Its able to converge at around 6000/7000 steps before steadily declining while not reseting converged earlier (around 4000/5000) but did not gain the benefits and declined more suddenly. In the end, reseting was very close to 0, being at -0.041; not reseting was lower at a value -1.165. Interestingly, both Thompson algorithms performed better than any of the epsilon greedy algorithms. I believe this shows a great advantage in these situations for the Thompson algorithm. 