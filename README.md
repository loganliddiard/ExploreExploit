# ExploreExploit

## Team Members:
    - Logan Liddiard
    - Joel Pierson

## Usage

`python main.py`

## Requirements:

    numpy
    matplotlib

These can be installed by running:
```
pip install numpy matplotlib
```

## Part 1

The epsilon value that produces the highest reward ratio on average is the epsilon of .01

We believe this is because once it has found when it believes is a better reward system, it is much less likely to change and go somewhere else.
On the flip side of this coin, the value that seems to perfom the worst is an epsiolon value of .4. This is simply because it is much more likley to explore even after it is found what it beleives is the best reward.

We can see that these algorithms all converge to their average reward fairly quickly. The greedy algorithm is dedicated to exploiting what it knows rather than exploring. This alogirthm doesn't explore all that much before sticking to what it thinks is the best.

Here is the output of one of our tests to prove this:

<p align="center">
  <img src="Epsilon_Test.png" alt="Epsilon Graph">
</p>

An alternative algorithm to this, is the Thompson algorithm, However it still doesn't match the power of even our best performing Epsilon algorithm. This is because 10,000 steps is much shorter comparativley than running this algorithm farther. The Thompson Sampling algorithm is made for long term decisions. It finds which probability has the most successes over failures. This algorithm tends to explore a bit more and focus more on what has given it the most successes. Finding these successors however, takes time. This can be seen by the flatter convergance curve on the Thompson Graph.

Here is a graph comparing our Epsilon Greedy algorithm of 0.01 vs. our Thompson Sampling algorithm:

<p align="center">
  <img src="Thompson_Test.png" alt="Thomspon+Epsilon Graph">
</p>

Upon further experimentation with this algorithm, we decided to run some tests with both 50,000 steps and 100,000 steps and over these longer periods of time, the Thompson sampling algorithm is infact better in the long run.
## Part 2

After successfully implementing the quenching functions for these Epsilon Greedy Algorithms, this was the result:

<p align="center">
  <img src="Quench_Test.png" alt="Quench Graph">
</p>

We can see based on the performance of linear Quenching that it is exploring too much. By constanty switching what reward system to use, it averages out pretty pourly for its average rewards it gets.

Now by looking at Asymptotpic Quenching, We see that the rewards are fairly high but the convergence to this average reward is flatter than Heavy Asymptotic

It seems that Heavy Asymptotpic Quenching seems to do that best as it's convergence rate is much steeper and it performs better over all. Showing us it has a pretty great ratio of exploring and exploiting, which gives us the better average reward. It also shows us that it's much quicker than the other algorithms at finding this since it converges in less steps than the others.
