import numpy as np
import matplotlib.pyplot as plt


# Thompson Sampling Algorithm
class ThompsonSampling:
    def __init__(self, n_actions):
        self.n_actions = n_actions
        self.successes = np.zeros(n_actions)
        self.failures = np.zeros(n_actions)

    def select_action(self):
        """Select an arm based on the current belief about each arm's success rate."""
        sampled_theta = [np.random.beta(1 + self.successes[i], 1 + self.failures[i]) for i in range(self.n_actions)]
        return np.argmax(sampled_theta)

    def update(self, action, reward):
        """Update the beliefs based on the outcome of pulling the arm."""
        if reward > 0:
            self.successes[action] += 1
        else:
            self.failures[action] += 1


class EpsilonGreedy:
    def __init__(self, n_arms, epsilon):
        self.n_arms = n_arms  # Number of arms (choices)
        self.epsilon = epsilon  # Exploration rate
        self.counts = np.zeros(n_arms)  # Number of times each arm was chosen
        self.values = np.zeros(n_arms)  # Estimated value of each arm

    def select_action(self):
        """Select an arm based on epsilon-greedy strategy."""
        if np.random.random() > self.epsilon:
            # Exploit: choose the arm with the highest estimated value
            return np.argmax(self.values)
        else:
            # Explore: choose a random arm
            return np.random.randint(self.n_arms)

    def update(self, chosen_arm, reward):
        """Update the chosen arm's value with the received reward."""
        self.counts[chosen_arm] += 1
        n = self.counts[chosen_arm]
        value = self.values[chosen_arm]
        # Incremental update to avoid recomputing sum of rewards
        new_value = ((n - 1) / n) * value + (1 / n) * reward
        self.values[chosen_arm] = new_value


def get_probabilities(drift=0):
    probs = [
        np.random.normal(0, 5),
        np.random.normal(-0.5, 12),
        np.random.normal(2, 3.9),
        np.random.normal(-0.5, 7),
        np.random.normal(-1.2, 8),
        np.random.normal(-3, 7),
        np.random.normal(-10, 20),
        np.random.normal(-0.5, 1),
        np.random.normal(-1, 2),
        np.random.normal(1, 6),
        np.random.normal(0.7, 4),
        np.random.normal(-6, 11),
        np.random.normal(-7, 1),
        np.random.normal(-0.5, 2),
        np.random.normal(-6.5, 1),
        np.random.normal(-3, 6),
        np.random.normal(0, 8),
        np.random.normal(2, 3.9),
        np.random.normal(-9, 12),
        np.random.normal(-1, 6),
        np.random.normal(-4.5, 8),
    ]
    return probs


if __name__ == "__main__":

    
    n_actions = 20
    steps = 10_000
    
    ##Part 1 Epsilon Greedy Algorithms

    epsilon_values = [0.01,0.05,0.1,0.4]  # chances of exploration
    avg_reward = []
    
    
    eps_idx = 0

    for epsilon in epsilon_values:
        total = 0
        step = 0
        

        avg_reward.append([])
        algo = EpsilonGreedy(n_actions, epsilon)

        # Simulate x steps rounds
        for _ in range(steps):
            action = algo.select_action()
            rewards = get_probabilities()
            reward = rewards[action]
            algo.update(action, reward)

            total += reward
            step += 1
            temp = total / step
            
            avg_reward[eps_idx].append(temp)

        print(f"Average rewards after {steps} steps for Epsilon {epsilon}: {temp}")
        ##increments which epsilon value we are working with
        eps_idx +=1

    y = range(0, steps)

    val = 0
    for i in avg_reward:
        x = i

        plt.plot(y, x,label=epsilon_values[val])
        val+=1

    plt.legend(title='Epsilon Values')

    plt.xlabel('Steps')
    plt.ylabel('Average Reward')
    plt.title('Epsilon Greedy: Average Reward Over Time')
    plt.savefig("Epsilon Greedy_performance.png")
    #plt.show()
    #print("Estimated values of actions:", algo.values)


    ##Part1 Thompson Algorithm
    
    avg_reward = []

    ##reset variables to work with
    algo = ThompsonSampling(n_actions)
    total = 0
    step = 0
    # Simulate x steps rounds
    for _ in range(steps):
        action = algo.select_action()  # Thompson Sampling chooses an arm
        rewards = get_probabilities()    # Get the reward from the environment
        reward = rewards[action]
        algo.update(action, reward) # Update beliefs based on the result

        total += reward
        step += 1
        temp = total / step
        
        avg_reward.append(temp)

    print(f"Average rewards after {steps} steps for Thomspon Sampling: {temp}")
    y = range(0, steps)
    x = avg_reward

    plt.plot(y,x)
    plt.xlabel('Steps')
    plt.ylabel('Average Reward')
    plt.title('Thompson Sampling: Average Reward Over Time')
    plt.savefig("Thompson_performance.png")
    #plt.show()

