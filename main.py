import numpy as np
import matplotlib.pyplot as plt


# Thompson Sampling Algorithm
class ThompsonSampling:
    def __init__(self, n_actions):
        self.n_actions = n_actions
        self.successes = np.zeros(n_actions)
        self.failures = np.zeros(n_actions)

    def select_action(self):
        """Select an arm based on the current belief about each arm's success rate."""
        sampled_theta = [np.random.beta(1 + self.successes[i], 1 + self.failures[i]) for i in range(self.n_actions)]
        return np.argmax(sampled_theta)

    def update(self, action, reward):
        """Update the beliefs based on the outcome of pulling the arm."""
        if reward > 0:
            self.successes[action] += 1
        else:
            self.failures[action] += 1


class EpsilonGreedy:
    def __init__(self, n_arms, epsilon):
        self.n_arms = n_arms  # Number of arms (choices)
        self.epsilon = epsilon  # Exploration rate
        self.counts = np.zeros(n_arms)  # Number of times each arm was chosen
        self.values = np.zeros(n_arms)  # Estimated value of each arm

        self.total_steps = 0

    def select_action(self):
        """Select an arm based on epsilon-greedy strategy."""
        if np.random.random() > self.epsilon:
            # Exploit: choose the arm with the highest estimated value
            return np.argmax(self.values)
        else:
            # Explore: choose a random arm
            return np.random.randint(self.n_arms)

    def update(self, chosen_arm, reward):
        """Update the chosen arm's value with the received reward."""
        self.counts[chosen_arm] += 1
        n = self.counts[chosen_arm]
        value = self.values[chosen_arm]
        # Incremental update to avoid recomputing sum of rewards
        new_value = ((n - 1) / n) * value + (1 / n) * reward
        self.values[chosen_arm] = new_value

    ##QUENCHING
    def set_total_steps(self,total_steps): self.total_steps = total_steps
    def linear_quench(self,t): self.epsilon = max(0, 1 - t / self.total_steps)
    def asymptotic_quench(self,t): self.epsilon = 1 / (1 + 0.01 * t)
    def heavy_asymptotic_quench(self,t): self.epsilon = 1 / (1 + 0.0001 * t**2)


def get_probabilities(drift=0):
    probs = [
        np.random.normal(0, 5),
        np.random.normal(-0.5, 12),
        np.random.normal(2, 3.9),
        np.random.normal(-0.5, 7),
        np.random.normal(-1.2, 8),
        np.random.normal(-3, 7),
        np.random.normal(-10, 20),
        np.random.normal(-0.5, 1),
        np.random.normal(-1, 2),
        np.random.normal(1, 6),
        np.random.normal(0.7, 4),
        np.random.normal(-6, 11),
        np.random.normal(-7, 1),
        np.random.normal(-0.5, 2),
        np.random.normal(-6.5, 1),
        np.random.normal(-3, 6),
        np.random.normal(0, 8),
        np.random.normal(2, 3.9),
        np.random.normal(-9, 12),
        np.random.normal(-1, 6),
        np.random.normal(-4.5, 8),
    ]
    return probs

def make_graph(x,steps,name,labels=None):
    y = range(0, steps)

    val = 0

    #only for graphs with various epsilons
    if labels != None:
        for i in x:
            current_x = i

            plt.plot(y, current_x,label=labels[val])
            val+=1

        plt.legend(title='Epsilon Values')
    #Graph everything else
    else: plt.plot(y,x)

    plt.xlabel('Steps')
    plt.ylabel('Average Reward')
    plt.title(f'{name}: Average Reward Over Time')
    plt.savefig(f"{name}_performance.png")
    plt.cla()

class MovingBandits:
    def __init__(self):
        # Initialize means based on original get_probabilities values
        self.probs = [
            0,     # Bandit 0
            -0.5,  # Bandit 1
            2,     # Bandit 2
            -0.5,  # Bandit 3
            -1.2,  # Bandit 4
            -3,    # Bandit 5
            -10,   # Bandit 6
            -0.5,  # Bandit 7
            -1,    # Bandit 8
            1,     # Bandit 9
            0.7,   # Bandit 10
            -6,    # Bandit 11
            -7,    # Bandit 12
            -0.5,  # Bandit 13
            -6.5,  # Bandit 14
            -3,    # Bandit 15
            0,     # Bandit 16
            2,     # Bandit 17
            -9,    # Bandit 18
            -1,    # Bandit 19
            -4.5,  # Bandit 20
        ]

        # keep std_devs so they are consistent when changing means
        self.std_devs = [
            5,    
            12,   
            3.9,  
            7,    
            8,    
            7,    
            20,   
            1,    
            2,    
            6,    
            4,  
            11, 
            1,  
            2,  
            1,  
            6,  
            8,  
            3.9,
            12, 
            6,  
            8   
        ]
        # Tracks when 3000 is reached
        self.shifted_mean_7 = None

    def apply_drift(self, step, drift=-0.001):
        # Apply drift to all bandit means
        self.probs = [mean + drift for mean in self.probs]

        # Apply sudden shifts at step 3,000
        if step == 3000:
            self.probs[0] += 7
            self.probs[2] += 3
            self.probs[7] += 1
            self.probs[18] += 2
            self.shifted_mean_7 = self.probs[7]

    def get_probabilities(self):
        # Return the current reward probabilities based on the updated means.
        rewards = []
        for i in range(len(self.probs)):
            if i == 7 and self.shifted_mean_7 is not None:
                # Special rule for Bandit 7: Check if it exceeds 3 standard deviations
                if abs(self.probs[7] - self.shifted_mean_7) > 3 * self.std_devs[7]:
                    rewards.append(50)  # Reward is set to 50 if it's more than 3 std away
                else:
                    rewards.append(np.random.normal(self.probs[7], self.std_devs[7]))
            else:
                rewards.append(np.random.normal(self.probs[i], self.std_devs[i]))
        return rewards
    
def make_graph(x,steps,name,epsilon_values=None):
    y = range(0, steps)

    val = 0

    #only for graphs with various epsilons
    if epsilon_values != None:
        for i in x:
            current_x = i

            plt.plot(y, current_x,label=epsilon_values[val])
            val+=1

        plt.legend(title='Epsilon Values')
    #Graph everything else
    else: plt.plot(y,x)

    plt.xlabel('Steps')
    plt.ylabel('Average Reward')
    plt.title(f'{name}: Average Reward Over Time')
    plt.savefig(f"{name}_performance.png")
    plt.cla()

def run_sim(steps,algo,name,epsilon_values=None,quench=None, bandits=None, thompson=None):
    avg_reward = []
    total = 0
    step = 0

    # Simulate x steps rounds
    for _ in range(steps):

        if quench:
            match quench:

                case "linear":
                    algo.linear_quench(step)
                    
                case "asymptotic":

                    algo.asymptotic_quench(step)
                case "heavy_asymptotic":
                    algo.heavy_asymptotic_quench(step)
                    
        
        action = algo.select_action()
        if bandits:
            rewards = bandits.get_probabilities()
            bandits.apply_drift(step)
        else:
            rewards = get_probabilities()
        reward = rewards[action]
        algo.update(action, reward)
        if bandits and step == 3000 and thompson:
            algo.successes = np.zeros(algo.n_actions)
            algo.failures = np.zeros(algo.n_actions)
            print(f"Thompson Sampling reset at step {step}")
        total += reward
        step += 1
        temp = total / step ## gives us the average reward at that step to record for avg_awards array
        
        avg_reward.append(temp)

    if epsilon_values and not bandits: print(f"Average rewards after {steps} steps for Epsilon {epsilon}: {temp}") 
    elif epsilon_values and bandits: print(f"Moving Bandits - Average rewards after {steps} steps for Epsilon {epsilon}: {temp}")
    elif not epsilon_values and bandits: print(f"Moving Bandits - Average rewards after {steps} steps for {name}: {temp}")
    else: print(f"Average rewards after {steps} steps for {name}: {temp}") 


    return avg_reward

if __name__ == "__main__":

    n_actions = 20
    steps = 10_000
    
    ## Part 1 Epsilon Greedy Algorithms
    epsilon_values = [0.01, 0.05, 0.1, 0.4]  # chances of exploration
    avg_reward = []

    for epsilon in epsilon_values:
        algo = EpsilonGreedy(n_actions, epsilon)
        new_array = run_sim(steps, algo, "Epsilon Greedy", epsilon_values)
        avg_reward.append(new_array)

    make_graph(avg_reward, steps, "Epsilon Greedy", epsilon_values)

    ## Part 1 Thompson Algorithm
    algo = ThompsonSampling(n_actions)
    avg_reward_thompson = run_sim(steps, algo, "Thompson Sampling", thompson=True)
    make_graph(avg_reward_thompson, steps, "Thompson Sampling")

    ## Part 2 Quenching
    ## Linear Quenching
    algo = EpsilonGreedy(n_actions, 1)
    algo.set_total_steps(steps)
    avg_reward_linear = run_sim(steps, algo, "Epsilon Greedy - Linear Quenching", None, "linear")
    make_graph(avg_reward_linear, steps, "Epsilon Greedy - Linear Quenching")

    ## Asymptotic Quenching
    algo = EpsilonGreedy(n_actions, 1)
    algo.set_total_steps(steps)
    avg_reward_asymptotic = run_sim(steps, algo, "Epsilon Greedy - Asymptotic Quenching", None, "asymptotic")
    make_graph(avg_reward_asymptotic, steps, "Epsilon Greedy - Asymptotic Quenching")

    ## Heavy Asymptotic Quenching
    algo = EpsilonGreedy(n_actions, 1)
    algo.set_total_steps(steps)
    avg_reward_heavy_asymptotic = run_sim(steps, algo, "Epsilon Greedy - Heavy Asymptotic Quenching", None, "heavy_asymptotic")
    make_graph(avg_reward_heavy_asymptotic, steps, "Epsilon Greedy - Heavy Asymptotic Quenching")

    ## Part 3 Moving Bandits
    avg_rewards_moving_bandits = []  # To collect results for each epsilon

    for epsilon in epsilon_values:
        bandits = MovingBandits()
        algo = EpsilonGreedy(n_actions, epsilon)
        avg_reward_eg_dynamic = run_sim(steps, algo, "Moving Bandits - Epsilon Greedy", epsilon_values, bandits=bandits)
        avg_rewards_moving_bandits.append(avg_reward_eg_dynamic)
    make_graph(avg_rewards_moving_bandits, steps, "Epsilon Greedy - Moving Bandits", epsilon_values)

    ## Thompson Sampling with Moving Bandits
    bandits = MovingBandits()
    thompson_algo = ThompsonSampling(n_actions)
    avg_reward_thompson_dynamic = run_sim(steps, thompson_algo, "Moving Bandits - Thompson Sampling", bandits=bandits, thompson=True)
    make_graph(avg_reward_thompson_dynamic, steps, "Thompson Sampling - Moving Bandits")