import numpy as np
import matplotlib.pyplot as plt


# Thompson Sampling Algorithm
class ThompsonSampling:
    def __init__(self, n_actions):
        self.n_actions = n_actions
        self.successes = np.zeros(n_actions)
        self.failures = np.zeros(n_actions)

    def select_action(self):
        """Select an arm based on the current belief about each arm's success rate."""
        sampled_theta = [np.random.beta(1 + self.successes[i], 1 + self.failures[i]) for i in range(self.n_actions)]
        return np.argmax(sampled_theta)

    def update(self, action, reward):
        """Update the beliefs based on the outcome of pulling the arm."""
        if reward > 0:
            self.successes[action] += 1
        else:
            self.failures[action] += 1


class EpsilonGreedy:
    def __init__(self, n_arms, epsilon):
        self.n_arms = n_arms  # Number of arms (choices)
        self.epsilon = epsilon  # Exploration rate
        self.counts = np.zeros(n_arms)  # Number of times each arm was chosen
        self.values = np.zeros(n_arms)  # Estimated value of each arm

        self.total_steps = 0

    def select_action(self):
        """Select an arm based on epsilon-greedy strategy."""
        if np.random.random() > self.epsilon:
            # Exploit: choose the arm with the highest estimated value
            return np.argmax(self.values)
        else:
            # Explore: choose a random arm
            return np.random.randint(self.n_arms)

    def update(self, chosen_arm, reward):
        """Update the chosen arm's value with the received reward."""
        self.counts[chosen_arm] += 1
        n = self.counts[chosen_arm]
        value = self.values[chosen_arm]
        # Incremental update to avoid recomputing sum of rewards
        new_value = ((n - 1) / n) * value + (1 / n) * reward
        self.values[chosen_arm] = new_value

    ##QUENCHING
    def set_total_steps(self,total_steps): self.total_steps = total_steps
    def linear_quench(self,t): self.epsilon = max(0, 1 - t / self.total_steps)
    def asymptotic_quench(self,t): self.epsilon = 1 / (1 + 0.01 * t)
    def heavy_asymptotic_quench(self,t): self.epsilon = 1 / (1 + 0.0001 * t**2)


def get_probabilities(drift=0):
    probs = [
        np.random.normal(0, 5),
        np.random.normal(-0.5, 12),
        np.random.normal(2, 3.9),
        np.random.normal(-0.5, 7),
        np.random.normal(-1.2, 8),
        np.random.normal(-3, 7),
        np.random.normal(-10, 20),
        np.random.normal(-0.5, 1),
        np.random.normal(-1, 2),
        np.random.normal(1, 6),
        np.random.normal(0.7, 4),
        np.random.normal(-6, 11),
        np.random.normal(-7, 1),
        np.random.normal(-0.5, 2),
        np.random.normal(-6.5, 1),
        np.random.normal(-3, 6),
        np.random.normal(0, 8),
        np.random.normal(2, 3.9),
        np.random.normal(-9, 12),
        np.random.normal(-1, 6),
        np.random.normal(-4.5, 8),
    ]
    return probs

def make_graph(x,steps,name,epsilon_values=None):
    y = range(0, steps)

    val = 0

    #only for graphs with various epsilons
    if epsilon_values != None:
        for i in x:
            current_x = i

            plt.plot(y, current_x,label=epsilon_values[val])
            val+=1

        plt.legend(title='Epsilon Values')
    #Graph everything else
    else: plt.plot(y,x)

    plt.xlabel('Steps')
    plt.ylabel('Average Reward')
    plt.title(f'{name}: Average Reward Over Time')
    plt.savefig(f"{name}_performance.png")
    plt.cla()

def run_sim(steps,algo,name,epsilon_values=None,quench=None):
    avg_reward = []

    total = 0
    step = 0

    # Simulate x steps rounds
    for _ in range(steps):

        if quench != None:
            match quench:

                case "linear":
                    algo.linear_quench(step)
                    
                case "asymptotic":

                    algo.asymptotic_quench(step)
                case "heavy_asymptotic":
                    algo.heavy_asymptotic_quench(step)
                    

        action = algo.select_action()
        rewards = get_probabilities()
        reward = rewards[action]
        algo.update(action, reward)

        total += reward
        step += 1
        temp = total / step ## gives us the average reward at that step to record for avg_awards array
        
        avg_reward.append(temp)

    if epsilon_values!= None: print(f"Average rewards after {steps} steps for Epsilon {epsilon}: {temp}") 

    else: print(f"Average rewards after {steps} steps for {name}: {temp}") 


    return avg_reward

if __name__ == "__main__":

    
    n_actions = 20
    steps = 10_000
    
##Part 1 Epsilon Greedy Algorithms

    epsilon_values = [0.01,0.05,0.1,0.4]  # chances of exploration
    avg_reward = []
    
    
    eps_idx = 0

    for epsilon in epsilon_values:
        total = 0
        step = 0
        

        #avg_reward.append([])
        algo = EpsilonGreedy(n_actions, epsilon)

        # Simulate x steps rounds
        new_array = run_sim(steps,algo,"Epsilon Greedy",epsilon_values)
        avg_reward.append(new_array)

        ##increments which epsilon value we are working with
        eps_idx +=1

    make_graph(avg_reward,steps,"Epsilon Greedy",epsilon_values)
    #print("Estimated values of actions:", algo.values)


##Part 1 Thompson Algorithm

    ##reset variables to work with
    algo = ThompsonSampling(n_actions)

    # Simulate x steps rounds
    avg_reward = run_sim(steps,algo,"Thompson Sampling")
    make_graph(avg_reward,steps,"Thompson Sampling")

##Part 2 Quenching
    ##Linear Quenching
    algo = EpsilonGreedy(n_actions, 1)
    algo.set_total_steps(steps)
    avg_reward = run_sim(steps,algo,"Epsilon Greedy - Linear Quenching",None,"linear")
 
    make_graph(avg_reward,steps,"Epsilon Greedy - Linear Quenching")

    ##Asymptotic
    algo = EpsilonGreedy(n_actions, 1)
    algo.set_total_steps(steps)
    avg_reward = run_sim(steps,algo,"Epsilon Greedy - Asymptotic Quenching",None,"asymptotic")
 
    make_graph(avg_reward,steps,"Epsilon Greedy - Asymptotic Quenching")

    ##Heavy Asymptotic
    algo = EpsilonGreedy(n_actions, 1)
    algo.set_total_steps(steps)
    avg_reward = run_sim(steps,algo,"Epsilon Greedy - Heavy Asymptotic Quenching",None,"heavy_asymptotic")
 
    make_graph(avg_reward,steps,"Epsilon Greedy - Heavy Asymptotic Quenching")



